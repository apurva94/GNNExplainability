import time
import json
import os

import torch
import numpy as np
from tqdm import tqdm

from ExplanationEvaluation.datasets.dataset_loaders import load_dataset
from ExplanationEvaluation.datasets.ground_truth_loaders import load_dataset_ground_truth
from ExplanationEvaluation.evaluation.AUCEvaluation import AUCEvaluation
from ExplanationEvaluation.evaluation.EfficiencyEvaluation import EfficiencyEvluation
#from ExplanationEvaluation.explainers.GNNExplainer import GNNExplainer
from ExplanationEvaluation.explainers.PGExplainer import PGExplainer
from ExplanationEvaluation.models.model_selector import model_selector
from ExplanationEvaluation.utils.plotting import plot, save_connectivity_graph
import random


def get_classification_task(graphs):
    """
    Given the original data, determines if the task as hand is a node or graph classification task
    :return: str either 'graph' or 'node'
    """
    if isinstance(graphs, list): # We're working with a model for graph classification
        return "graph"
    else:
        return "node"


def to_torch_graph(graphs, task):
    """
    Transforms the numpy graphs to torch tensors depending on the task of the model that we want to explain
    :param graphs: list of single numpy graph
    :param task: either 'node' or 'graph'
    :return: torch tensor
    """
    if task == 'graph':
        return [torch.tensor(g) for g in graphs]
    else:
        return torch.tensor(graphs)


def select_explainer(explainer, model, graphs, features, task, folder, epochs, lr, reg_coefs, temp=None, sample_bias=None):
    """
    Select the explainer we which to use.
    :param explainer: str, "PG" or "GNN"
    :param model: graph classification model who's predictions we wish to explain.
    :param graphs: the collections of edge_indices representing the graphs
    :param features: the collcection of features for each node in the graphs.
    :param task: str "node" or "graph"
    :param epochs: amount of epochs to train our explainer
    :param lr: learning rate used in the training of the explainer
    :param reg_coefs: reguaization coefficients used in the loss. The first item in the tuple restricts the size of the explainations, the second rescticts the entropy matrix mask.
    :param temp: the temperture parameters dictacting how we sample our random graphs.
    :params sample_bias: the bias we add when sampling random graphs.
    """
    if explainer == "PG":
        return PGExplainer(model, graphs, features, task, folder, epochs=epochs, lr=lr, reg_coefs=reg_coefs, temp=temp, sample_bias=sample_bias)
 #   elif explainer == "GNN":
 #       return GNNExplainer(model, graphs, features, task, epochs=epochs, lr=lr, reg_coefs=reg_coefs)
    else:
        raise NotImplementedError("Unknown explainer type")


def run_experiment(inference_eval, auc_eval, explainer, indices, seed):
    """
    Runs an experiment.
    We generate explanations for given indices and calculate the AUC score.
    :param inference_eval: object for measure the inference speed
    :param auc_eval: a metric object, which calculate the AUC score
    :param explainer: the explainer we wish to obtain predictions from
    :param indices: indices over which to evaluate the auc
    :returns: AUC score, inference speed
    """
    inference_eval.start_prepate()
    explainer.prepare(indices=indices, seed=seed)

    inference_eval.start_explaining()
    explanations = []
    for idx in tqdm(indices):
        graph, expl = explainer.explain(idx)
        explanations.append((graph, expl))
    inference_eval.done_explaining()

    auc_score = auc_eval.get_score(explanations)
    time_score = inference_eval.get_score(explanations)

    return auc_score, time_score


def run_qualitative_experiment(explainer, indices, labels, config, explanation_labels):
    """
    Plot the explaination generated by the explainer
    :param explainer: the explainer object
    :param indices: indices on which we validate
    :param labels: predictions of the explainer
    :param config: dict holding which subgraphs to plot
    :param explanation_labels: the ground truth labels
    """
    for idx in indices:
        graph, expl = explainer.explain(idx)
        plot(graph, expl, labels, idx, config.thres_min, config.thres_snip, config.dataset, config, explanation_labels)


def run_connectivity_experiment(explainer, indices, labels, config, explanation_labels, seed):
    """
    Plot the explaination generated by the explainer
    :param explainer: the explainer object
    :param indices: indices on which we validate
    :param labels: predictions of the explainer
    :param config: dict holding which subgraphs to plot
    :param explanation_labels: the ground truth labels
    """
    final_score_rand = 0
    final_score_shuffled = 0
    for idx in indices:
        graph, expl = explainer.explain(idx)
        #max_c, positions,
        area = save_connectivity_graph(graph, expl, labels, idx, config.thres_min, config.dataset, seed, config, explanation_labels, folder_name = 'numberOfConnectedComponents_'+str(config.log_folder))
        #pos = min(positions)

        # print("max",torch.max(expl))
        # print("min",torch.min(expl))
        # print("shape", expl.shape[0])
        # print(np.random.uniform(torch.min(expl).item(),torch.max(expl).item(),expl.shape[0]))
        random_edge_weights = np.random.uniform(torch.min(expl).item(),torch.max(expl).item(),expl.shape[0])
        # max_rand, positions_rand,
        area_rand = save_connectivity_graph(graph, torch.FloatTensor(random_edge_weights), labels, idx, config.thres_min, config.dataset, seed, config, explanation_labels,folder_name = 'randomNumberOfConnectedComponents'+str(config.log_folder))
        #print(positions_rand)
        #pos_rand = max(positions_rand)

        rand_indx = torch.randperm(len(expl))
        shuffled_edge_weights = expl[rand_indx]
        # print(expl)
        # print(shuffled_edge_weights)
        #max_shuffled, positions_shuffled,
        area_shuffled = save_connectivity_graph(graph, shuffled_edge_weights, labels, idx, config.thres_min, config.dataset, seed, config, explanation_labels,folder_name = 'shuffledNumberOfConnectedComponents'+str(config.log_folder))
        #pos_shuffled = max(positions_shuffled)

        score = []
        #score.append(max_c - max_rand)
        #score.append(max_c - max_shuffled)
        #score.append(pos - pos_rand)
        #score.append(pos - pos_shuffled)
        score.append(area / area_rand)
        score.append(area / area_shuffled)
        final_score_rand += area / area_rand
        final_score_shuffled += area / area_shuffled
    final_score_rand /= len(indices)
    final_score_shuffled /= len(indices)

        # print("Connectivity against baseline",score)
    return final_score_rand, final_score_shuffled




def store_results(auc, auc_std, inf_time, area_rand, area_shuffled, checkpoint, config):
    """
    Save the replication results into a json file
    :param auc: the obtained AUC score
    :param auc_std: the obtained AUC standard deviation
    :param inf_time: time it takes to make a single prediction
    :param checkpoint: the checkpoint of the explained model
    :param config: dict config
    """
    results = {"AUC": auc,
               "AUC std": auc_std,
               "Inference time (ms)": inf_time,
               "ratio of connected area with random": area_rand,
               "ratio of connected area with shuffled": area_rand}

    model_res = {"Training Accuracy": checkpoint["train_acc"],
                 "Validation Accuracy": checkpoint["val_acc"],
                 "Test Accuracy": checkpoint["test_acc"], }

    explainer_params = {"Explainer": config.explainer,
                        "Model": config.model,
                        "Dataset": config.dataset}

    json_dict = {"Explainer parameters": explainer_params,
                 "Results": results,
                 "Trained model stats": model_res}

    save_dir = "./results"
    os.makedirs(save_dir, exist_ok=True)
    with open(f"./results/P_{config.explainer}_M_{config.model}_D_{config.dataset}_results.json", "w") as fp:
        json.dump(json_dict, fp, indent=4)


def replication(config, extension=False, run_qual=True, results_store=True):
    """
    Perform the replication study.
    First load a pre-trained model.
    Then we train our expainer.
    Followed by obtaining the generated explanations.
    And saving the obtained AUC score in a json file.
    :param config: a dict containing the config file values
    :param extension: bool, wheter to use all indices
    """
    # Load complete dataset
    graphs, features, labels, _, _, test_mask = load_dataset(config.dataset)
    task = get_classification_task(graphs)

    features = torch.tensor(features)
    labels = torch.tensor(labels)
    graphs = to_torch_graph(graphs, task)

    # Load pretrained models
    model, checkpoint = model_selector(config.model,
                                        config.dataset,
                                        pretrained=False,
                                        return_checkpoint=True)
    if config.eval_enabled:
        model.eval()

    # Get ground_truth for every node
    explanation_labels, indices = load_dataset_ground_truth(config.dataset)
    if extension: indices = np.argwhere(test_mask).squeeze()

    # Get explainer
    explainer = select_explainer(config.explainer,
                                 model=model,
                                 graphs=graphs,
                                 features=features,
                                 task=task,
                                 folder=config.log_folder,
                                 epochs=config.epochs,
                                 lr=config.lr,
                                 reg_coefs=[config.reg_size,
                                            config.reg_ent,
                                            config.reg_conn],
                                 temp=config.temps,
                                 sample_bias=config.sample_bias)

    # Get evaluation methods
    auc_evaluation = AUCEvaluation(task, explanation_labels, indices)
    inference_eval = EfficiencyEvluation()

    # Perform the evaluation 10 times
    auc_scores = []
    times = []
    area_scores_rand = []
    area_scores_shuffled = []

    for idx, s in enumerate(config.seeds):
        print(f"Run {idx} with seed {s}")
        # Set all seeds needed for reproducibility
        torch.manual_seed(s)
        torch.cuda.manual_seed(s)
        np.random.seed(s)

        inference_eval.reset()
        auc_score, time_score = run_experiment(inference_eval, auc_evaluation, explainer, indices, s)

        area_score_rand, area_score_shuffled = run_connectivity_experiment(explainer, indices, labels, config, explanation_labels, s)

        if idx == 0 and run_qual: # We only run the qualitative experiment once
            run_qualitative_experiment(explainer, indices, labels, config, explanation_labels)

        auc_scores.append(auc_score)
        print("score:",auc_score)
        times.append(time_score)
        print("time_elased:",time_score)
        area_scores_rand.append(area_score_rand)
        print("connectivity_score with random:", area_score_rand)
        area_scores_shuffled.append(area_score_rand)
        print("connectivity_score with shuffled:", area_score_shuffled)

    auc = np.mean(auc_scores)
    auc_std = np.std(auc_scores)
    inf_time = np.mean(times) / 10
    area_rand = np.mean(area_scores_rand)
    area_shuffled = np.mean(area_scores_shuffled)
    print("FINAL AREA SCORE", area_rand, area_shuffled)

    if results_store: store_results(auc, auc_std, inf_time, area_rand, area_shuffled, checkpoint, config)

    return (auc, auc_std), inf_time
